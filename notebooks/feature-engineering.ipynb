{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ada187",
   "metadata": {},
   "source": [
    "# Feature Engineering and Data Preparation\n",
    "\n",
    "This notebook transforms cleaned transaction datasets into feature-rich,\n",
    "model-ready formats. It includes time-based features, transaction behavior\n",
    "features, scaling, encoding, and proper handling of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d538600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the neccessary dependecies...\n",
      "succussfully imported\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing the neccessary dependecies...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(\"succussfully imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34395861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from processed and raw file\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading from processed and raw file\")\n",
    "fraud_df = pd.read_csv(\"./data/processed/fraud_cleaned.csv\")\n",
    "credit_df = pd.read_csv(\"./data/processed/creditcard_cleaned.csv\")\n",
    "ip_df = pd.read_csv(\"./data/raw/IpAddress_to_Country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f68d8",
   "metadata": {},
   "source": [
    "## Fraud_Data Feature Engineering\n",
    "\n",
    "We begin by converting timestamps and creating time-based features\n",
    "that capture user behavior and transaction timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3996834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to datetime...\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting to datetime...\")\n",
    "fraud_df[\"signup_time\"] = pd.to_datetime(fraud_df[\"signup_time\"])\n",
    "fraud_df[\"purchase_time\"] = pd.to_datetime(fraud_df[\"purchase_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5a6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time since signup\n"
     ]
    }
   ],
   "source": [
    "print(\"Time since signup\")\n",
    "fraud_df[\"time_since_signup\"] = (\n",
    "    fraud_df[\"purchase_time\"] - fraud_df[\"signup_time\"]\n",
    ").dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88159fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour and day features\n"
     ]
    }
   ],
   "source": [
    "print(\"Hour and day features\")\n",
    "fraud_df[\"hour_of_day\"] = fraud_df[\"purchase_time\"].dt.hour\n",
    "fraud_df[\"day_of_week\"] = fraud_df[\"purchase_time\"].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311f955",
   "metadata": {},
   "source": [
    "### Transaction Velocity\n",
    "\n",
    "Fraudulent users often perform multiple transactions in a short time window.\n",
    "We compute transaction frequency per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c716c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting by user and time\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorting by user and time\")\n",
    "fraud_df = fraud_df.sort_values([\"user_id\" , \"purchase_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06689b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions per user\n"
     ]
    }
   ],
   "source": [
    "print(\"Transactions per user\")\n",
    "fraud_df[\"user_transaction_count\"] = fraud_df.groupby(\"user_id\")[\"purchase_time\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd118e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time betweein tranasactions\n"
     ]
    }
   ],
   "source": [
    "print(\"Time betweein tranasactions\")\n",
    "fraud_df[\"time_since_last_tx\"] = fraud_df.groupby(\"user_id\")[\"purchase_time\"].diff().dt.total_seconds()\n",
    "fraud_df[\"time_since_last_tx\"] = fraud_df[\"time_since_last_tx\"].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437970d2",
   "metadata": {},
   "source": [
    "### Country Feature\n",
    "\n",
    "We enrich the dataset by mapping IP addresses to countries.\n",
    "This geographic signal can improve fraud detection accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "366afd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ip conversion...\n"
     ]
    }
   ],
   "source": [
    "print(\"Ip conversion...\")\n",
    "fraud_df[\"ip_address\"] = fraud_df[\"ip_address\"].astype(int)\n",
    "ip_df[\"lower_bound_ip_address\"] = ip_df[\"lower_bound_ip_address\"].astype(int)\n",
    "ip_df[\"upper_bound_ip_address\"] = ip_df[\"upper_bound_ip_address\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dae0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP mapping function\n"
     ]
    }
   ],
   "source": [
    "print(\"IP mapping function\")\n",
    "def map_ip_to_country(ip):\n",
    "    match = ip_df[\n",
    "        (ip_df[\"lower_bound_ip_address\"] <= ip) &\n",
    "        (ip_df[\"upper_bound_ip_address\"] >= ip)\n",
    "    ]\n",
    "    return match.iloc[0][\"country\"] if not match.empty else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93574291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying country mapping\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying country mapping\")\n",
    "fraud_df[\"country\"] = fraud_df[\"ip_address\"].apply(map_ip_to_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e2429",
   "metadata": {},
   "source": [
    "### Leakage Prevention\n",
    "\n",
    "Raw timestamps and identifiers are removed to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ba0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop unused columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Drop unused columns\")\n",
    "fraud_df_model = fraud_df.drop(\n",
    "    columns=[\"signup_time\", \"purchase_time\", \"ip_address\", \"device_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a9acd",
   "metadata": {},
   "source": [
    "## PART B — Preprocessing Pipeline (Fraud_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0518b19",
   "metadata": {},
   "source": [
    "## Fraud_Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b8503",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling (Fraud_Data)\n",
    "\n",
    "SMOTE was not applied due to the presence of high-cardinality categorical features.\n",
    "Applying SMOTE would require generating artificial categorical values, which is\n",
    "statistically invalid.\n",
    "\n",
    "Instead, class-weighted models are used to handle imbalance while preserving\n",
    "data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7491dca",
   "metadata": {},
   "source": [
    "## Class imbalance handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b936ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fraud_df_model.drop(\"class\", axis=1)\n",
    "y = fraud_df_model[\"class\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "158417a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aad1474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature types\")\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bed20454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column transformer\n"
     ]
    }
   ],
   "source": [
    "print(\"Column transformer\")\n",
    "preprocessor_fraud = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933aa707",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Stratified splitting preserves fraud distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7db7d3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\n",
      "Train fraud ratio:  <bound method Series.mean of 145492    0\n",
      "116211    0\n",
      "81635     0\n",
      "109796    0\n",
      "38621     0\n",
      "         ..\n",
      "147940    0\n",
      "75408     0\n",
      "66453     0\n",
      "9251      0\n",
      "113722    0\n",
      "Name: class, Length: 120889, dtype: int64>\n",
      "Test fraud ratio: 0.09363729609899746\n"
     ]
    }
   ],
   "source": [
    "print(\"Split\")\n",
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Train fraud ratio: \", yf_train.mean)\n",
    "print(\"Test fraud ratio:\", yf_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17f34754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features scaled successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_cols = [\n",
    "    \"purchase_value\",\n",
    "    \"age\",\n",
    "    \"time_since_signup\",\n",
    "    \"user_transaction_count\",\n",
    "    \"time_since_last_tx\"\n",
    "]\n",
    "\n",
    "Xf_train[num_cols] = scaler.fit_transform(Xf_train[num_cols])\n",
    "Xf_test[num_cols] = scaler.transform(Xf_test[num_cols])\n",
    "\n",
    "print(\"Numerical features scaled successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "552beb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'log_amount', 'hour'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(Xf_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a080ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (120889, 12)\n",
      "X_test shape: (30223, 12)\n",
      "y_train distribution:\n",
      " class\n",
      "0    109568\n",
      "1     11321\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", Xf_train.shape)\n",
    "print(\"X_test shape:\", Xf_test.shape)\n",
    "print(\"y_train distribution:\\n\", yf_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7109f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'log_amount', 'hour']\n"
     ]
    }
   ],
   "source": [
    "print(Xf_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ba2ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "categorical_features = X.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "febe6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete\n"
     ]
    }
   ],
   "source": [
    "Xf_train[\"log_amount\"] = np.log1p(Xf_train[\"Amount\"])\n",
    "Xf_test[\"log_amount\"] = np.log1p(Xf_test[\"Amount\"])\n",
    "\n",
    "Xf_train[\"hour\"] = (Xf_train[\"Time\"] // 3600) % 24\n",
    "Xf_test[\"hour\"] = (Xf_test[\"Time\"] // 3600) % 24\n",
    "\n",
    "print(\"Feature engineering complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed8bbf",
   "metadata": {},
   "source": [
    "## PART C — Feature Prep for Credit Card Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8eae6",
   "metadata": {},
   "source": [
    "## Credit Card Dataset Preparation\n",
    "\n",
    "PCA features are already standardized.\n",
    "Only `Time` and `Amount` require scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f321196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target and features\n"
     ]
    }
   ],
   "source": [
    "X_credit = credit_df.drop(\"Class\", axis=1)\n",
    "y_credit = credit_df[\"Class\"]\n",
    "print(\"Target and features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d7b48c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test split\n"
     ]
    }
   ],
   "source": [
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
    "    X_credit, y_credit,\n",
    "    test_size=0.2,\n",
    "    stratify=y_credit,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Train-Test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f9713c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Amount & Time\n",
      "Scaling complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Scale Amount & Time\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cols_to_scale = [\"Amount\", \"Time\"]\n",
    "\n",
    "Xc_train[cols_to_scale] = scaler.fit_transform(Xc_train[cols_to_scale])\n",
    "Xc_test[cols_to_scale] = scaler.transform(Xc_test[cols_to_scale])\n",
    "\n",
    "print(\"Scaling complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b76f1d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(Xc_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42abccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete\n"
     ]
    }
   ],
   "source": [
    "Xf_train[\"log_amount\"] = np.log1p(Xf_train[\"Amount\"])\n",
    "Xf_test[\"log_amount\"] = np.log1p(Xf_test[\"Amount\"])\n",
    "\n",
    "Xf_train[\"hour\"] = (Xf_train[\"Time\"] // 3600) % 24\n",
    "Xf_test[\"hour\"] = (Xf_test[\"Time\"] // 3600) % 24\n",
    "\n",
    "print(\"Feature engineering complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4498517",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "Completed:\n",
    "- Time-based features\n",
    "- Transaction velocity features\n",
    "- Geographic enrichment\n",
    "- Scaling and encoding\n",
    "- Class imbalance handling\n",
    "- Leakage prevention\n",
    "\n",
    "Datasets are now fully prepared for modeling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
